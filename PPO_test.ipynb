{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_pickle('./data/features_test.pkl')\n",
    "stock_data = pd.read_csv('./data/stock_data_test.csv', index_col='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'4584.T':           feat_1      feat_2     feat_3      feat_4      feat_5      feat_6  \\\n",
       " 180   734.689087  123.499207  30.843113 -345.803986  433.608459 -449.363556   \n",
       " 181   634.747009  103.663925  28.473894 -294.388885  373.312225 -384.783264   \n",
       " 182   676.864746  112.292336  32.621391 -316.591156  401.190735 -412.538086   \n",
       " 183   694.850098  118.005768  34.235714 -324.470490  412.803833 -425.951752   \n",
       " 184   684.634338  116.040680  32.078613 -317.571198  406.474396 -417.737030   \n",
       " ...          ...         ...        ...         ...         ...         ...   \n",
       " 1294  223.451508   39.885864  11.101674 -102.470436  138.206482 -143.450790   \n",
       " 1295  230.035980   42.742805   9.579368 -105.303726  140.051514 -141.415268   \n",
       " 1296  226.375488   37.585320  12.369206 -103.315849  140.573196 -144.255249   \n",
       " 1297  230.569122   39.036400  13.432136 -105.590363  142.691833 -142.796387   \n",
       " 1298  226.544662   41.507702  13.038601 -102.488571  138.436844 -143.210434   \n",
       " \n",
       "           feat_7      feat_8      feat_9     feat_10  exog_feat_1  \\\n",
       " 180  -200.893875 -645.468689 -737.342224  428.996552 -2051.018066   \n",
       " 181  -174.953903 -554.103638 -634.737000  369.530579 -2115.369385   \n",
       " 182  -185.043198 -593.416382 -682.187134  396.991913 -2120.232178   \n",
       " 183  -192.627029 -613.934082 -699.962646  407.030670 -2193.758057   \n",
       " 184  -192.142899 -602.709717 -690.230286  398.648499 -2088.319092   \n",
       " ...          ...         ...         ...         ...          ...   \n",
       " 1294  -65.406441 -204.659821 -228.054810  130.900284 -4058.105225   \n",
       " 1295  -65.160324 -207.203323 -234.271332  134.621109 -3730.643555   \n",
       " 1296  -66.568909 -202.890076 -230.990753  135.066879 -3847.061279   \n",
       " 1297  -67.065880 -204.451401 -233.041870  137.824203 -4029.982178   \n",
       " 1298  -63.789005 -206.721161 -228.121368  134.641083 -3827.526367   \n",
       " \n",
       "       exog_feat_2  exog_feat_3  exog_feat_4   exog_feat_5  exog_feat_6  \\\n",
       " 180  -1644.951050 -2979.258789  -315.531250  -5662.730469  2971.260986   \n",
       " 181  -1649.798828 -2915.382812  -331.799561  -5739.345703  2929.878906   \n",
       " 182  -1678.105347 -2916.890869  -307.365784  -5795.995117  2830.669434   \n",
       " 183  -1529.763428 -2991.667236  -377.524414  -5850.974121  2939.077393   \n",
       " 184  -1707.751465 -3002.937744  -258.150238  -5875.478516  3001.312744   \n",
       " ...           ...          ...          ...           ...          ...   \n",
       " 1294 -7243.617188    99.619270  9414.447266 -11783.809570 -5952.700195   \n",
       " 1295 -7597.217285   354.342041  9540.713867 -11401.334961 -5999.856934   \n",
       " 1296 -7632.402344   439.117126  9685.764648 -11374.024414 -6129.157715   \n",
       " 1297 -7559.902344   320.407532  9392.168945 -11300.159180 -5884.375488   \n",
       " 1298 -7610.331543   671.911865  9257.266602 -11365.633789 -5718.160645   \n",
       " \n",
       "        exog_feat_7   exog_feat_8  exog_feat_9  exog_feat_10  \n",
       " 180    9883.273438   7771.602539 -3712.602539  -1962.086792  \n",
       " 181   10040.206055   7916.118164 -3712.121338  -1761.489136  \n",
       " 182   10007.979492   8077.133789 -3709.203857  -1986.183228  \n",
       " 183    9911.701172   8172.082520 -3737.843506  -1976.125854  \n",
       " 184   10050.087891   8130.104980 -3805.400146  -2021.010986  \n",
       " ...            ...           ...          ...           ...  \n",
       " 1294  21396.056641  16066.654297 -1122.322388  -3905.011719  \n",
       " 1295  21217.710938  15912.000977 -1012.679810  -3766.694336  \n",
       " 1296  21388.453125  15685.375977  -923.956848  -3900.574707  \n",
       " 1297  20916.050781  15556.413086  -896.886902  -3863.054199  \n",
       " 1298  21248.324219  15601.906250  -990.495605  -3905.381348  \n",
       " \n",
       " [1119 rows x 20 columns],\n",
       " '1557.T':             feat_1       feat_2       feat_3        feat_4        feat_5  \\\n",
       " 180   27538.425781  4608.321777  1211.916748 -12899.758789  16265.434570   \n",
       " 181   27609.714844  4617.136719  1217.092529 -12928.541016  16306.276367   \n",
       " 182   27922.548828  4671.061523  1233.168823 -13077.537109  16494.056641   \n",
       " 183   28091.673828  4702.063965  1241.442993 -13156.207031  16594.943359   \n",
       " 184   28341.652344  4743.635254  1250.751099 -13271.172852  16742.296875   \n",
       " ...            ...          ...          ...           ...           ...   \n",
       " 1294  53190.964844  8902.451172  2345.049805 -24910.644531  31423.861328   \n",
       " 1295  53222.421875  8909.469727  2344.623291 -24925.126953  31440.398438   \n",
       " 1296  53258.937500  8911.035156  2349.183594 -24941.955078  31464.648438   \n",
       " 1297  53183.734375  8899.201172  2346.747803 -24907.042969  31419.871094   \n",
       " 1298  53230.406250  8910.155273  2348.588379 -24927.687500  31445.562500   \n",
       " \n",
       "             feat_6        feat_7        feat_8        feat_9       feat_10  \\\n",
       " 180  -16691.132812  -7565.832031 -24064.492188 -27674.177734  16090.828125   \n",
       " 181  -16730.308594  -7586.941406 -24122.734375 -27743.652344  16131.414062   \n",
       " 182  -16922.105469  -7671.416016 -24398.578125 -28063.164062  16317.058594   \n",
       " 183  -17027.101562  -7720.529297 -24551.152344 -28232.830078  16415.410156   \n",
       " 184  -17176.552734  -7791.539062 -24767.263672 -28484.583984  16559.064453   \n",
       " ...            ...           ...           ...           ...           ...   \n",
       " 1294 -32239.201172 -14619.438477 -46483.562500 -53458.589844  31080.623047   \n",
       " 1295 -32252.236328 -14626.026367 -46507.835938 -53489.800781  31098.876953   \n",
       " 1296 -32279.421875 -14638.474609 -46538.625000 -53526.894531  31122.796875   \n",
       " 1297 -32229.851562 -14617.154297 -46470.816406 -53449.156250  31079.162109   \n",
       " 1298 -32260.986328 -14627.808594 -46517.382812 -53495.187500  31105.603516   \n",
       " \n",
       "       exog_feat_1  exog_feat_2  exog_feat_3   exog_feat_4   exog_feat_5  \\\n",
       " 180  -1610.202637 -1541.060913 -3549.060059    415.315918  -5578.323242   \n",
       " 181  -1671.737671 -1545.245117 -3488.823975    403.716553  -5654.399414   \n",
       " 182  -1672.148315 -1572.502319 -3496.087158    435.531799  -5710.196289   \n",
       " 183  -1743.188477 -1423.574463 -3574.076660    369.494263  -5764.699219   \n",
       " 184  -1633.470215 -1600.554199 -3590.878418    495.963043  -5788.384277   \n",
       " ...           ...          ...          ...           ...           ...   \n",
       " 1294 -3186.998779 -7038.317383 -1026.378784  10858.692383 -11617.009766   \n",
       " 1295 -2859.128906 -7391.821777  -772.184814  10985.637695 -11234.458008   \n",
       " 1296 -2974.884521 -7426.850586  -688.263733  11131.783203 -11207.019531   \n",
       " 1297 -3159.112061 -7354.658203  -805.285339  10836.022461 -11133.405273   \n",
       " 1298 -2955.822266 -7404.891113  -454.859161  10702.502930 -11198.719727   \n",
       " \n",
       "       exog_feat_6   exog_feat_7   exog_feat_8  exog_feat_9  exog_feat_10  \n",
       " 180   3767.392334  10100.478516   8439.305664 -3895.438965  -1507.067261  \n",
       " 181   3731.096191  10258.798828   8588.084961 -3896.125732  -1303.562744  \n",
       " 182   3639.927734  10228.767578   8755.844727 -3895.054932  -1523.661133  \n",
       " 183   3752.824951  10133.712891   8854.558594 -3924.725342  -1511.037964  \n",
       " 184   3822.788574  10274.208008   8819.062500 -3994.056885  -1551.506226  \n",
       " ...           ...           ...           ...          ...           ...  \n",
       " 1294 -4379.445312  21825.283203  17386.117188 -1483.629028  -3005.836914  \n",
       " 1295 -4425.863281  21647.138672  17232.083984 -1374.156250  -2867.097168  \n",
       " 1296 -4553.970703  21818.207031  17006.458984 -1285.707764  -3000.295410  \n",
       " 1297 -4311.546387  21345.162109  16875.517578 -1258.096802  -2964.122559  \n",
       " 1298 -4143.826172  21677.843750  16922.275391 -1352.051270  -3005.588867  \n",
       " \n",
       " [1119 rows x 20 columns],\n",
       " '8789.T':           feat_1     feat_2    feat_3     feat_4     feat_5     feat_6  \\\n",
       " 180   132.035446  22.662724  4.287927 -63.541637  77.646606 -84.184616   \n",
       " 181   133.492249  19.793583  6.386721 -59.618279  77.242386 -81.047134   \n",
       " 182   133.519867  21.379444  8.679571 -62.107014  80.260078 -83.297417   \n",
       " 183   135.243134  24.371897  9.577325 -62.369717  82.267838 -86.857010   \n",
       " 184   135.549942  24.167440  7.883886 -60.398796  82.153595 -85.018425   \n",
       " ...          ...        ...       ...        ...        ...        ...   \n",
       " 1294   69.440018  14.116544  4.315348 -30.336725  47.238461 -50.127285   \n",
       " 1295   74.111313  16.653368  2.708739 -32.273941  47.953461 -46.932465   \n",
       " 1296   71.407410  11.655943  5.540730 -30.734106  49.040157 -50.352116   \n",
       " 1297   63.165329  11.026268  6.055694 -27.184153  43.813557 -41.357800   \n",
       " 1298   61.054066  13.817689  5.746461 -24.978434  40.688602 -42.931160   \n",
       " \n",
       "          feat_7      feat_8      feat_9    feat_10  exog_feat_1  exog_feat_2  \\\n",
       " 180  -35.301056 -118.916771 -131.696045  76.856888 -2060.929688 -1647.286987   \n",
       " 181  -37.222733 -116.146187 -130.993134  76.639801 -2123.613037 -1651.741699   \n",
       " 182  -35.746811 -118.683853 -136.144211  79.507256 -2129.168213 -1680.211304   \n",
       " 183  -38.862270 -124.992966 -137.576935  80.043846 -2202.961182 -1531.932373   \n",
       " 184  -41.269444 -122.962448 -138.419342  77.810135 -2097.349365 -1709.879883   \n",
       " ...         ...         ...         ...        ...          ...          ...   \n",
       " 1294 -23.088272  -70.096542  -73.278564  40.909035 -4060.637451 -7244.213867   \n",
       " 1295 -22.316463  -70.968460  -77.572403  43.511967 -3733.208008 -7597.821777   \n",
       " 1296 -23.987894  -67.491020  -75.253159  44.516685 -3849.609131 -7633.002930   \n",
       " 1297 -21.067873  -58.186970  -64.806831  40.007626 -4032.735107 -7560.550781   \n",
       " 1298 -18.316689  -62.128330  -61.809006  37.942413 -3830.248047 -7610.973145   \n",
       " \n",
       "       exog_feat_3  exog_feat_4   exog_feat_5  exog_feat_6   exog_feat_7  \\\n",
       " 180  -2966.447266  -331.963623  -5664.628418  2953.360596   9878.388672   \n",
       " 181  -2904.726807  -345.467041  -5740.924805  2914.990234  10036.143555   \n",
       " 182  -2905.340332  -322.180969  -5797.706543  2814.531006  10003.577148   \n",
       " 183  -2979.770996  -392.783081  -5852.736328  2922.455811   9907.166016   \n",
       " 184  -2991.265137  -273.121918  -5877.208008  2985.003662  10045.637695   \n",
       " ...           ...          ...           ...          ...           ...   \n",
       " 1294   102.893684  9410.248047 -11784.293945 -5957.274902  21394.810547   \n",
       " 1295   357.656494  9536.461914 -11401.826172 -6004.488281  21216.447266   \n",
       " 1296   442.411560  9681.539062 -11374.511719 -6133.760742  21387.197266   \n",
       " 1297   323.966125  9387.604492 -11300.686523 -5889.347656  20914.695312   \n",
       " 1298   675.429932  9252.754883 -11366.154297 -5723.076172  21246.982422   \n",
       " \n",
       "        exog_feat_8  exog_feat_9  exog_feat_10  \n",
       " 180    7756.589844 -3708.491699  -1972.317505  \n",
       " 181    7903.631348 -3708.702393  -1769.998413  \n",
       " 182    8063.598633 -3705.497803  -1995.407104  \n",
       " 183    8158.142090 -3734.026611  -1985.625732  \n",
       " 184    8116.427246 -3801.654541  -2030.332275  \n",
       " ...            ...          ...           ...  \n",
       " 1294  16062.816406 -1121.271606  -3907.626465  \n",
       " 1295  15908.117188 -1011.616333  -3769.341309  \n",
       " 1296  15681.514648  -922.900208  -3903.205566  \n",
       " 1297  15552.242188  -895.745300  -3865.895996  \n",
       " 1298  15597.784180  -989.366699  -3908.190430  \n",
       " \n",
       " [1119 rows x 20 columns],\n",
       " '1893.T':           feat_1      feat_2     feat_3      feat_4      feat_5      feat_6  \\\n",
       " 180   508.576294   85.665840  20.879734 -239.900497  300.053284 -312.350159   \n",
       " 181   512.902893   83.276871  23.104982 -237.321243  301.344116 -310.951630   \n",
       " 182   519.861694   86.022461  25.703243 -243.056305  308.455719 -317.401855   \n",
       " 183   502.917358   85.891434  25.778435 -234.575745  299.437347 -309.649811   \n",
       " 184   521.332336   88.716858  24.882912 -241.086075  310.018860 -318.783905   \n",
       " ...          ...         ...        ...         ...         ...         ...   \n",
       " 1294  638.612915  109.350998  29.395247 -296.917847  383.424652 -395.018494   \n",
       " 1295  648.067139  112.688110  27.999395 -301.095245  386.964722 -394.721924   \n",
       " 1296  647.276428  108.010803  30.915688 -300.451477  389.181458 -399.300873   \n",
       " 1297  642.860718  108.021355  31.599255 -298.693665  386.214935 -392.625153   \n",
       " 1298  644.575806  111.453003  31.458628 -298.280060  385.350037 -396.517120   \n",
       " \n",
       "           feat_7      feat_8      feat_9     feat_10  exog_feat_1  \\\n",
       " 180  -138.764236 -447.908936 -510.106720  296.875427 -2054.736816   \n",
       " 181  -141.474457 -447.645752 -512.287842  298.335205 -2117.373291   \n",
       " 182  -141.903015 -456.239319 -524.404480  305.252655 -2122.814209   \n",
       " 183  -139.889145 -446.238159 -507.076904  294.881470 -2196.914551   \n",
       " 184  -147.271957 -460.029144 -526.117432  303.228668 -2091.004639   \n",
       " ...          ...         ...         ...         ...          ...   \n",
       " 1294 -179.481476 -567.395569 -645.277710  373.485382 -4051.277100   \n",
       " 1295 -180.023911 -572.446472 -654.378296  378.883057 -3723.768555   \n",
       " 1296 -182.221024 -570.640625 -653.981689  381.005676 -3840.138428   \n",
       " 1297 -180.352386 -564.679749 -647.380798  378.732452 -4023.201904   \n",
       " 1298 -178.652603 -571.964355 -648.228394  378.903046 -3820.651367   \n",
       " \n",
       "       exog_feat_2  exog_feat_3  exog_feat_4   exog_feat_5  exog_feat_6  \\\n",
       " 180  -1645.827515 -2974.451904  -321.696655  -5663.442383  2964.544678   \n",
       " 181  -1650.270996 -2912.792480  -335.121826  -5739.729492  2926.259766   \n",
       " 182  -1678.713745 -2913.553223  -311.646667  -5796.489746  2826.006104   \n",
       " 183  -1530.507080 -2987.587158  -382.757812  -5851.578125  2933.376465   \n",
       " 184  -1708.384277 -2999.466309  -262.602875  -5875.993164  2996.462402   \n",
       " ...           ...          ...          ...           ...          ...   \n",
       " 1294 -7242.007812    90.793587  9425.767578 -11782.501953 -5940.369141   \n",
       " 1295 -7595.597168   345.455322  9552.112305 -11400.018555 -5987.440430   \n",
       " 1296 -7630.770508   430.169373  9697.241211 -11372.698242 -6116.655762   \n",
       " 1297 -7558.303711   311.642883  9403.411133 -11298.860352 -5872.129395   \n",
       " 1298 -7608.711426   663.025146  9268.665039 -11364.317383 -5705.744141   \n",
       " \n",
       "        exog_feat_7   exog_feat_8  exog_feat_9  exog_feat_10  \n",
       " 180    9881.440430   7765.969727 -3711.060059  -1965.925293  \n",
       " 181   10039.217773   7913.082520 -3711.290283  -1763.557495  \n",
       " 182   10006.708008   8073.222656 -3708.133057  -1988.848511  \n",
       " 183    9910.145508   8167.301270 -3736.534424  -1979.384033  \n",
       " 184   10048.763672   8126.037109 -3804.286377  -2023.783203  \n",
       " ...            ...           ...          ...           ...  \n",
       " 1294  21399.421875  16076.996094 -1125.154419  -3897.964355  \n",
       " 1295  21221.097656  15922.415039 -1015.531372  -3759.598145  \n",
       " 1296  21391.865234  15695.860352  -926.827942  -3893.429688  \n",
       " 1297  20919.392578  15566.683594  -899.699402  -3856.055176  \n",
       " 1298  21251.710938  15612.319336  -993.347168  -3898.284668  \n",
       " \n",
       " [1119 rows x 20 columns],\n",
       " 'MSFT':             feat_1       feat_2       feat_3        feat_4        feat_5  \\\n",
       " 180   10279.118164  1720.480469   451.406586  -4816.087891   6071.095703   \n",
       " 181   10642.860352  1778.229370   469.469086  -4981.847168   6284.678711   \n",
       " 182   10403.351562  1739.735840   461.207062  -4872.145020   6146.212402   \n",
       " 183   10223.040039  1712.270142   454.083679  -4787.148926   6040.700195   \n",
       " 184   10576.238281  1771.111694   467.939972  -4950.460449   6249.023926   \n",
       " ...            ...          ...          ...           ...           ...   \n",
       " 1294  39912.660156  6680.715820  1759.957764 -18691.541016  23580.933594   \n",
       " 1295  39665.019531  6641.035645  1747.233276 -18575.302734  23432.619141   \n",
       " 1296  40413.609375  6761.745117  1783.170044 -18925.642578  23877.460938   \n",
       " 1297  39773.945312  6655.464844  1755.862183 -18626.355469  23499.279297   \n",
       " 1298  39748.039062  6654.275879  1754.504639 -18613.005859  23482.101562   \n",
       " \n",
       "             feat_6        feat_7        feat_8        feat_9       feat_10  \\\n",
       " 180   -6232.825684  -2823.443115  -8984.649414 -10329.165039   6005.951172   \n",
       " 181   -6449.216309  -2924.911621  -9298.416016 -10692.547852   6217.423340   \n",
       " 182   -6306.318359  -2857.617188  -9091.665039 -10456.972656   6080.325684   \n",
       " 183   -6199.574219  -2810.714844  -8938.926758 -10275.467773   5974.497070   \n",
       " 184   -6411.570801  -2910.086914  -9245.225586 -10630.953125   6178.462891   \n",
       " ...            ...           ...           ...           ...           ...   \n",
       " 1294 -24193.191406 -10970.921875 -34882.011719 -40114.347656  23321.908203   \n",
       " 1295 -24037.107422 -10900.821289 -34662.429688 -39865.078125  23177.082031   \n",
       " 1296 -24495.773438 -11108.926758 -35315.375000 -40617.777344  23617.076172   \n",
       " 1297 -24104.167969 -10932.508789 -34754.382812 -39972.777344  23243.619141   \n",
       " 1298 -24091.324219 -10923.220703 -34737.539062 -39945.871094  23227.650391   \n",
       " \n",
       "       exog_feat_1   exog_feat_2   exog_feat_3   exog_feat_4   exog_feat_5  \\\n",
       " 180    725.439148  -6864.336426  -3739.760498 -13444.205078  -9380.388672   \n",
       " 181    733.192993  -7018.342773  -3732.882080 -13390.204102  -9462.022461   \n",
       " 182    617.334412  -6814.483887  -3667.211182 -13361.724609  -9723.606445   \n",
       " 183    496.712524  -6805.148926  -3572.921875 -13403.119141  -9817.151367   \n",
       " 184    592.106323  -6919.795898  -3581.221436 -13628.063477  -9927.935547   \n",
       " ...           ...           ...           ...           ...           ...   \n",
       " 1294  -348.762329 -19820.242188 -15400.330078 -36327.839844 -21728.292969   \n",
       " 1295  -473.812866 -19642.369141 -15252.024414 -36182.882812 -21490.259766   \n",
       " 1296  -818.299194 -19860.068359 -15440.520508 -36273.542969 -21659.007812   \n",
       " 1297  -785.450806 -19694.453125 -15336.036133 -35885.500000 -21151.082031   \n",
       " 1298  -590.081177 -19816.947266 -15387.566406 -36249.035156 -21232.541016   \n",
       " \n",
       "       exog_feat_6   exog_feat_7  exog_feat_8   exog_feat_9  exog_feat_10  \n",
       " 180  -3776.258545    512.080261  3280.419434   5638.772461   -137.396027  \n",
       " 181  -3933.604736    361.983765  3350.037354   5619.998047    -63.515480  \n",
       " 182  -4046.842285    709.790222  3330.574463   5842.145020   -141.121414  \n",
       " 183  -4062.722412    694.728455  3349.649902   5969.952637   -225.791595  \n",
       " 184  -4074.547363    741.498779  3333.327148   6016.973633   -295.896759  \n",
       " ...           ...           ...          ...           ...           ...  \n",
       " 1294   740.576050 -13391.488281 -4851.071289  12611.798828   4196.157715  \n",
       " 1295   509.548462 -13350.405273 -4804.732422  12346.977539   4159.367188  \n",
       " 1296   542.158325 -13600.157227 -4794.036621  12571.556641   3857.194824  \n",
       " 1297   314.133362 -13453.722656 -4806.938965  12402.046875   3822.311768  \n",
       " 1298   309.235291 -13573.929688 -4701.265137  12564.713867   3847.767822  \n",
       " \n",
       " [1119 rows x 20 columns]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>4584.T</th>\n",
       "      <th>1557.T</th>\n",
       "      <th>8789.T</th>\n",
       "      <th>1893.T</th>\n",
       "      <th>MSFT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>661.0</td>\n",
       "      <td>28860.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>533.62700</td>\n",
       "      <td>11123.248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>708.0</td>\n",
       "      <td>29190.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>543.87270</td>\n",
       "      <td>10875.842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>730.0</td>\n",
       "      <td>29370.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>529.35803</td>\n",
       "      <td>10690.548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>718.0</td>\n",
       "      <td>29630.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>547.28790</td>\n",
       "      <td>11058.452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>732.0</td>\n",
       "      <td>29800.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>552.41077</td>\n",
       "      <td>11266.821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1294</th>\n",
       "      <td>243.0</td>\n",
       "      <td>55640.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>680.00000</td>\n",
       "      <td>41467.410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1295</th>\n",
       "      <td>241.0</td>\n",
       "      <td>55680.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>681.00000</td>\n",
       "      <td>42251.793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1296</th>\n",
       "      <td>244.0</td>\n",
       "      <td>55600.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>675.00000</td>\n",
       "      <td>41581.720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1297</th>\n",
       "      <td>241.0</td>\n",
       "      <td>55650.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>678.00000</td>\n",
       "      <td>41555.848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1298</th>\n",
       "      <td>241.0</td>\n",
       "      <td>55650.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>678.00000</td>\n",
       "      <td>41555.848</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1119 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      4584.T   1557.T  8789.T     1893.T       MSFT\n",
       "180    661.0  28860.0   137.0  533.62700  11123.248\n",
       "181    708.0  29190.0   140.0  543.87270  10875.842\n",
       "182    730.0  29370.0   145.0  529.35803  10690.548\n",
       "183    718.0  29630.0   144.0  547.28790  11058.452\n",
       "184    732.0  29800.0   141.0  552.41077  11266.821\n",
       "...      ...      ...     ...        ...        ...\n",
       "1294   243.0  55640.0    80.0  680.00000  41467.410\n",
       "1295   241.0  55680.0    79.0  681.00000  42251.793\n",
       "1296   244.0  55600.0    69.0  675.00000  41581.720\n",
       "1297   241.0  55650.0    68.0  678.00000  41555.848\n",
       "1298   241.0  55650.0    68.0  678.00000  41555.848\n",
       "\n",
       "[1119 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(features)\n",
    "display(stock_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from ray.tune import register_env\n",
    "from ray.rllib.models import ModelCatalog\n",
    "\n",
    "model_name = \"CustumMoldel\"\n",
    "env_name = \"stock_trading_env\"\n",
    "\n",
    "# カスタムモデルのインポート\n",
    "from MyCustomModel import MaskModel\n",
    "\n",
    "ModelCatalog.register_custom_model(model_name, MaskModel)\n",
    "\n",
    "# 環境クラスのインポート\n",
    "from gym_stock_trading_env import StockTradingEnv\n",
    "\n",
    "# Define a function to create the environment\n",
    "def create_stock_trading_env(_):\n",
    "    env = StockTradingEnv(stock_data, features)\n",
    "    return env\n",
    "\n",
    "# Register the environment\n",
    "register_env(env_name, create_stock_trading_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-01 08:56:11,060\tINFO worker.py:1625 -- Started a local Ray instance.\n",
      "2023-06-01 08:56:13,484\tINFO algorithm.py:527 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=8380)\u001b[0m 2023-06-01 08:56:16,436\tWARNING env.py:155 -- Your env doesn't have a .spec.max_episode_steps attribute. Your horizon will default to infinity, and your environment will not be reset.\n",
      "2023-06-01 08:56:16,517\tERROR actor_manager.py:507 -- Ray error, taking actor 1 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=8380, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x000001AD00BE3340>)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 877, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 881, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 821, in ray._raylet.execute_task.function_executor\n",
      "  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 670, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 460, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 738, in __init__\n",
      "    self._update_policy_map(policy_dict=self.policy_dict)\n",
      "  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 460, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 1985, in _update_policy_map\n",
      "    self._build_policy_map(\n",
      "  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 460, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 2097, in _build_policy_map\n",
      "    new_policy = create_policy_for_framework(\n",
      "  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\utils\\policy.py\", line 142, in create_policy_for_framework\n",
      "    return policy_class(observation_space, action_space, merged_config)\n",
      "  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\algorithms\\ppo\\ppo_torch_policy.py\", line 67, in __init__\n",
      "    self._initialize_loss_from_dummy_batch()\n",
      "  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\policy\\policy.py\", line 1405, in _initialize_loss_from_dummy_batch\n",
      "    actions, state_outs, extra_outs = self.compute_actions_from_input_dict(\n",
      "  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\policy\\torch_policy_v2.py\", line 522, in compute_actions_from_input_dict\n",
      "    return self._compute_action_helper(\n",
      "  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\utils\\threading.py\", line 24, in wrapper\n",
      "    return func(self, *a, **k)\n",
      "  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\policy\\torch_policy_v2.py\", line 1141, in _compute_action_helper\n",
      "    dist_inputs, state_out = self.model(input_dict, state_batches, seq_lens)\n",
      "  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\models\\modelv2.py\", line 274, in __call__\n",
      "    raise ValueError(\"State output is not a list: {}\".format(state_out))\n",
      "ValueError: State output is not a list: Box(-1.0, 1.0, (171,), float32)\n",
      "2023-06-01 08:56:16,517\tERROR actor_manager.py:507 -- Ray error, taking actor 2 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=19088, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x00000179E0323370>)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 877, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 881, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 821, in ray._raylet.execute_task.function_executor\n",
      "  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 670, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 460, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 738, in __init__\n",
      "    self._update_policy_map(policy_dict=self.policy_dict)\n",
      "  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 460, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 1985, in _update_policy_map\n",
      "    self._build_policy_map(\n",
      "  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 460, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 2097, in _build_policy_map\n",
      "    new_policy = create_policy_for_framework(\n",
      "  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\utils\\policy.py\", line 142, in create_policy_for_framework\n",
      "    return policy_class(observation_space, action_space, merged_config)\n",
      "  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\algorithms\\ppo\\ppo_torch_policy.py\", line 67, in __init__\n",
      "    self._initialize_loss_from_dummy_batch()\n",
      "  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\policy\\policy.py\", line 1405, in _initialize_loss_from_dummy_batch\n",
      "    actions, state_outs, extra_outs = self.compute_actions_from_input_dict(\n",
      "  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\policy\\torch_policy_v2.py\", line 522, in compute_actions_from_input_dict\n",
      "    return self._compute_action_helper(\n",
      "  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\utils\\threading.py\", line 24, in wrapper\n",
      "    return func(self, *a, **k)\n",
      "  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\policy\\torch_policy_v2.py\", line 1141, in _compute_action_helper\n",
      "    dist_inputs, state_out = self.model(input_dict, state_batches, seq_lens)\n",
      "  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\models\\modelv2.py\", line 274, in __call__\n",
      "    raise ValueError(\"State output is not a list: {}\".format(state_out))\n",
      "ValueError: State output is not a list: Box(-1.0, 1.0, (171,), float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=19088)\u001b[0m 4584.T : wrong_action selected !!!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19088)\u001b[0m 1557.T : wrong_action selected !!!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19088)\u001b[0m 1893.T : wrong_action selected !!!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19088)\u001b[0m num_outputs 65\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19088)\u001b[0m flat_obs torch.Size([32, 171])\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19088)\u001b[0m bsize 32\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19088)\u001b[0m action_logits torch.Size([32, 65])\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19088)\u001b[0m state (171,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=19088)\u001b[0m 2023-06-01 08:56:16,509\tERROR worker.py:844 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=19088, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x00000179E0323370>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19088)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 877, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19088)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 881, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19088)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 821, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19088)\u001b[0m   File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 670, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19088)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19088)\u001b[0m   File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 460, in _resume_span\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19088)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19088)\u001b[0m   File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 738, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19088)\u001b[0m     self._update_policy_map(policy_dict=self.policy_dict)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19088)\u001b[0m   File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 460, in _resume_span\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19088)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19088)\u001b[0m   File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 1985, in _update_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19088)\u001b[0m     self._build_policy_map(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19088)\u001b[0m   File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 460, in _resume_span\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19088)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19088)\u001b[0m   File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 2097, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19088)\u001b[0m     new_policy = create_policy_for_framework(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19088)\u001b[0m   File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\utils\\policy.py\", line 142, in create_policy_for_framework\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19088)\u001b[0m     return policy_class(observation_space, action_space, merged_config)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19088)\u001b[0m   File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\algorithms\\ppo\\ppo_torch_policy.py\", line 67, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19088)\u001b[0m     self._initialize_loss_from_dummy_batch()\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19088)\u001b[0m   File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\policy\\policy.py\", line 1405, in _initialize_loss_from_dummy_batch\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19088)\u001b[0m     actions, state_outs, extra_outs = self.compute_actions_from_input_dict(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19088)\u001b[0m   File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\policy\\torch_policy_v2.py\", line 522, in compute_actions_from_input_dict\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19088)\u001b[0m     return self._compute_action_helper(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19088)\u001b[0m   File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\utils\\threading.py\", line 24, in wrapper\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19088)\u001b[0m     return func(self, *a, **k)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19088)\u001b[0m   File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\policy\\torch_policy_v2.py\", line 1141, in _compute_action_helper\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19088)\u001b[0m     dist_inputs, state_out = self.model(input_dict, state_batches, seq_lens)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19088)\u001b[0m   File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\models\\modelv2.py\", line 274, in __call__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19088)\u001b[0m     raise ValueError(\"State output is not a list: {}\".format(state_out))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19088)\u001b[0m ValueError: State output is not a list: Box(-1.0, 1.0, (171,), float32)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "State output is not a list: Box(-1.0, 1.0, (171,), float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRayActorError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\evaluation\\worker_set.py:172\u001b[0m, in \u001b[0;36mWorkerSet.__init__\u001b[1;34m(self, env_creator, validate_env, default_policy_class, config, num_workers, local_worker, logdir, _setup, policy_class, trainer_config)\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 172\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_setup(\n\u001b[0;32m    173\u001b[0m         validate_env\u001b[39m=\u001b[39;49mvalidate_env,\n\u001b[0;32m    174\u001b[0m         config\u001b[39m=\u001b[39;49mconfig,\n\u001b[0;32m    175\u001b[0m         num_workers\u001b[39m=\u001b[39;49mnum_workers,\n\u001b[0;32m    176\u001b[0m         local_worker\u001b[39m=\u001b[39;49mlocal_worker,\n\u001b[0;32m    177\u001b[0m     )\n\u001b[0;32m    178\u001b[0m \u001b[39m# WorkerSet creation possibly fails, if some (remote) workers cannot\u001b[39;00m\n\u001b[0;32m    179\u001b[0m \u001b[39m# be initialized properly (due to some errors in the RolloutWorker's\u001b[39;00m\n\u001b[0;32m    180\u001b[0m \u001b[39m# constructor).\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\evaluation\\worker_set.py:242\u001b[0m, in \u001b[0;36mWorkerSet._setup\u001b[1;34m(self, validate_env, config, num_workers, local_worker)\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[39m# Create a number of @ray.remote workers.\u001b[39;00m\n\u001b[1;32m--> 242\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_workers(\n\u001b[0;32m    243\u001b[0m     num_workers,\n\u001b[0;32m    244\u001b[0m     validate\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mvalidate_workers_after_construction,\n\u001b[0;32m    245\u001b[0m )\n\u001b[0;32m    247\u001b[0m \u001b[39m# If num_workers > 0 and we don't have an env on the local worker,\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \u001b[39m# get the observation- and action spaces for each policy from\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[39m# the first remote worker (which does have an env).\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\evaluation\\worker_set.py:635\u001b[0m, in \u001b[0;36mWorkerSet.add_workers\u001b[1;34m(self, num_workers, validate)\u001b[0m\n\u001b[0;32m    634\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m result\u001b[39m.\u001b[39mok:\n\u001b[1;32m--> 635\u001b[0m     \u001b[39mraise\u001b[39;00m result\u001b[39m.\u001b[39mget()\n",
      "File \u001b[1;32mc:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\utils\\actor_manager.py:488\u001b[0m, in \u001b[0;36mFaultTolerantActorManager.__fetch_result\u001b[1;34m(self, remote_actor_ids, remote_calls, tags, timeout_seconds, return_obj_refs, mark_healthy)\u001b[0m\n\u001b[0;32m    487\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 488\u001b[0m     result \u001b[39m=\u001b[39m ray\u001b[39m.\u001b[39;49mget(r)\n\u001b[0;32m    489\u001b[0m     remote_results\u001b[39m.\u001b[39madd_result(actor_id, ResultOrError(result\u001b[39m=\u001b[39mresult), tag)\n",
      "File \u001b[1;32mc:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\_private\\client_mode_hook.py:105\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(ray, func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 105\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\_private\\worker.py:2523\u001b[0m, in \u001b[0;36mget\u001b[1;34m(object_refs, timeout)\u001b[0m\n\u001b[0;32m   2522\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2523\u001b[0m             \u001b[39mraise\u001b[39;00m value\n\u001b[0;32m   2525\u001b[0m \u001b[39mif\u001b[39;00m is_individual_id:\n",
      "\u001b[1;31mRayActorError\u001b[0m: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=8380, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x000001AD00BE3340>)\n  File \"python\\ray\\_raylet.pyx\", line 877, in ray._raylet.execute_task\n  File \"python\\ray\\_raylet.pyx\", line 881, in ray._raylet.execute_task\n  File \"python\\ray\\_raylet.pyx\", line 821, in ray._raylet.execute_task.function_executor\n  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 670, in actor_method_executor\n    return method(__ray_actor, *args, **kwargs)\n  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 460, in _resume_span\n    return method(self, *_args, **_kwargs)\n  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 738, in __init__\n    self._update_policy_map(policy_dict=self.policy_dict)\n  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 460, in _resume_span\n    return method(self, *_args, **_kwargs)\n  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 1985, in _update_policy_map\n    self._build_policy_map(\n  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 460, in _resume_span\n    return method(self, *_args, **_kwargs)\n  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 2097, in _build_policy_map\n    new_policy = create_policy_for_framework(\n  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\utils\\policy.py\", line 142, in create_policy_for_framework\n    return policy_class(observation_space, action_space, merged_config)\n  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\algorithms\\ppo\\ppo_torch_policy.py\", line 67, in __init__\n    self._initialize_loss_from_dummy_batch()\n  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\policy\\policy.py\", line 1405, in _initialize_loss_from_dummy_batch\n    actions, state_outs, extra_outs = self.compute_actions_from_input_dict(\n  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\policy\\torch_policy_v2.py\", line 522, in compute_actions_from_input_dict\n    return self._compute_action_helper(\n  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\utils\\threading.py\", line 24, in wrapper\n    return func(self, *a, **k)\n  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\policy\\torch_policy_v2.py\", line 1141, in _compute_action_helper\n    dist_inputs, state_out = self.model(input_dict, state_batches, seq_lens)\n  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\models\\modelv2.py\", line 274, in __call__\n    raise ValueError(\"State output is not a list: {}\".format(state_out))\nValueError: State output is not a list: Box(-1.0, 1.0, (171,), float32)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39m# ppo_config[\"lr\"] = 0.00002 # default : 5e-5\u001b[39;00m\n\u001b[0;32m     15\u001b[0m ppo_agent \u001b[39m=\u001b[39m ppo_config\u001b[39m.\u001b[39menvironment(env\u001b[39m=\u001b[39menv_name)\n\u001b[1;32m---> 16\u001b[0m ppo_agent \u001b[39m=\u001b[39m ppo_config\u001b[39m.\u001b[39;49mbuild()\n\u001b[0;32m     18\u001b[0m \u001b[39m# Track rewards\u001b[39;00m\n\u001b[0;32m     19\u001b[0m rewards \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm_config.py:1071\u001b[0m, in \u001b[0;36mAlgorithmConfig.build\u001b[1;34m(self, env, logger_creator, use_copy)\u001b[0m\n\u001b[0;32m   1068\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39malgo_class, \u001b[39mstr\u001b[39m):\n\u001b[0;32m   1069\u001b[0m     algo_class \u001b[39m=\u001b[39m get_trainable_cls(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39malgo_class)\n\u001b[1;32m-> 1071\u001b[0m \u001b[39mreturn\u001b[39;00m algo_class(\n\u001b[0;32m   1072\u001b[0m     config\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m use_copy \u001b[39melse\u001b[39;49;00m copy\u001b[39m.\u001b[39;49mdeepcopy(\u001b[39mself\u001b[39;49m),\n\u001b[0;32m   1073\u001b[0m     logger_creator\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlogger_creator,\n\u001b[0;32m   1074\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py:466\u001b[0m, in \u001b[0;36mAlgorithm.__init__\u001b[1;34m(self, config, env, logger_creator, **kwargs)\u001b[0m\n\u001b[0;32m    454\u001b[0m \u001b[39m# Initialize common evaluation_metrics to nan, before they become\u001b[39;00m\n\u001b[0;32m    455\u001b[0m \u001b[39m# available. We want to make sure the metrics are always present\u001b[39;00m\n\u001b[0;32m    456\u001b[0m \u001b[39m# (although their values may be nan), so that Tune does not complain\u001b[39;00m\n\u001b[0;32m    457\u001b[0m \u001b[39m# when we use these as stopping criteria.\u001b[39;00m\n\u001b[0;32m    458\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation_metrics \u001b[39m=\u001b[39m {\n\u001b[0;32m    459\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mevaluation\u001b[39m\u001b[39m\"\u001b[39m: {\n\u001b[0;32m    460\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mepisode_reward_max\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mnan,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    463\u001b[0m     }\n\u001b[0;32m    464\u001b[0m }\n\u001b[1;32m--> 466\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\n\u001b[0;32m    467\u001b[0m     config\u001b[39m=\u001b[39mconfig,\n\u001b[0;32m    468\u001b[0m     logger_creator\u001b[39m=\u001b[39mlogger_creator,\n\u001b[0;32m    469\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    470\u001b[0m )\n\u001b[0;32m    472\u001b[0m \u001b[39m# Check, whether `training_iteration` is still a tune.Trainable property\u001b[39;00m\n\u001b[0;32m    473\u001b[0m \u001b[39m# and has not been overridden by the user in the attempt to implement the\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[39m# algos logic (this should be done now inside `training_step`).\u001b[39;00m\n\u001b[0;32m    475\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\tune\\trainable\\trainable.py:169\u001b[0m, in \u001b[0;36mTrainable.__init__\u001b[1;34m(self, config, logger_creator, remote_checkpoint_dir, custom_syncer, sync_timeout, sync_config)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m    168\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_local_ip \u001b[39m=\u001b[39m ray\u001b[39m.\u001b[39mutil\u001b[39m.\u001b[39mget_node_ip_address()\n\u001b[1;32m--> 169\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msetup(copy\u001b[39m.\u001b[39;49mdeepcopy(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig))\n\u001b[0;32m    170\u001b[0m setup_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time\n\u001b[0;32m    171\u001b[0m \u001b[39mif\u001b[39;00m setup_time \u001b[39m>\u001b[39m SETUP_TIME_THRESHOLD:\n",
      "File \u001b[1;32mc:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py:592\u001b[0m, in \u001b[0;36mAlgorithm.setup\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[39m# Only if user did not override `_init()`:\u001b[39;00m\n\u001b[0;32m    586\u001b[0m \u001b[39mif\u001b[39;00m _init \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[0;32m    587\u001b[0m     \u001b[39m# - Create rollout workers here automatically.\u001b[39;00m\n\u001b[0;32m    588\u001b[0m     \u001b[39m# - Run the execution plan to create the local iterator to `next()`\u001b[39;00m\n\u001b[0;32m    589\u001b[0m     \u001b[39m#   in each training iteration.\u001b[39;00m\n\u001b[0;32m    590\u001b[0m     \u001b[39m# This matches the behavior of using `build_trainer()`, which\u001b[39;00m\n\u001b[0;32m    591\u001b[0m     \u001b[39m# has been deprecated.\u001b[39;00m\n\u001b[1;32m--> 592\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mworkers \u001b[39m=\u001b[39m WorkerSet(\n\u001b[0;32m    593\u001b[0m         env_creator\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv_creator,\n\u001b[0;32m    594\u001b[0m         validate_env\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalidate_env,\n\u001b[0;32m    595\u001b[0m         default_policy_class\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_default_policy_class(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig),\n\u001b[0;32m    596\u001b[0m         config\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig,\n\u001b[0;32m    597\u001b[0m         num_workers\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49mnum_rollout_workers,\n\u001b[0;32m    598\u001b[0m         local_worker\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    599\u001b[0m         logdir\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlogdir,\n\u001b[0;32m    600\u001b[0m     )\n\u001b[0;32m    602\u001b[0m     \u001b[39m# TODO (avnishn): Remove the execution plan API by q1 2023\u001b[39;00m\n\u001b[0;32m    603\u001b[0m     \u001b[39m# Function defining one single training iteration's behavior.\u001b[39;00m\n\u001b[0;32m    604\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39m_disable_execution_plan_api:\n\u001b[0;32m    605\u001b[0m         \u001b[39m# Ensure remote workers are initially in sync with the local worker.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\evaluation\\worker_set.py:194\u001b[0m, in \u001b[0;36mWorkerSet.__init__\u001b[1;34m(self, env_creator, validate_env, default_policy_class, config, num_workers, local_worker, logdir, _setup, policy_class, trainer_config)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[39mexcept\u001b[39;00m RayActorError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    182\u001b[0m     \u001b[39m# In case of an actor (remote worker) init failure, the remote worker\u001b[39;00m\n\u001b[0;32m    183\u001b[0m     \u001b[39m# may still exist and will be accessible, however, e.g. calling\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[39m# its `sample.remote()` would result in strange \"property not found\"\u001b[39;00m\n\u001b[0;32m    185\u001b[0m     \u001b[39m# errors.\u001b[39;00m\n\u001b[0;32m    186\u001b[0m     \u001b[39mif\u001b[39;00m e\u001b[39m.\u001b[39mactor_init_failed:\n\u001b[0;32m    187\u001b[0m         \u001b[39m# Raise the original error here that the RolloutWorker raised\u001b[39;00m\n\u001b[0;32m    188\u001b[0m         \u001b[39m# during its construction process. This is to enforce transparency\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    192\u001b[0m         \u001b[39m# - e.args[0].args[2]: The original Exception (e.g. a ValueError due\u001b[39;00m\n\u001b[0;32m    193\u001b[0m         \u001b[39m# to a config mismatch) thrown inside the actor.\u001b[39;00m\n\u001b[1;32m--> 194\u001b[0m         \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39margs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39margs[\u001b[39m2\u001b[39m]\n\u001b[0;32m    195\u001b[0m     \u001b[39m# In any other case, raise the RayActorError as-is.\u001b[39;00m\n\u001b[0;32m    196\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    197\u001b[0m         \u001b[39mraise\u001b[39;00m e\n",
      "\u001b[1;31mValueError\u001b[0m: State output is not a list: Box(-1.0, 1.0, (171,), float32)"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import ray\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from collections import defaultdict\n",
    "\n",
    "# Initialize Ray in local mode\n",
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "# Create PPO agent\n",
    "ppo_config = PPOConfig()\n",
    "ppo_config[\"model\"][\"custom_model\"] = model_name\n",
    "ppo_config[\"vf_loss_coeff\"] = 0.01 # default : False\n",
    "ppo_config[\"entropy_coeff_schedule\"] = [(0, 0.1), (3000, 0.001)]  # default : None\n",
    "# ppo_config[\"lr\"] = 0.00002 # default : 5e-5\n",
    "ppo_agent = ppo_config.environment(env=env_name)\n",
    "ppo_agent = ppo_config.build()\n",
    "\n",
    "# Track rewards\n",
    "rewards = []\n",
    "\n",
    "# Train the agents\n",
    "num_iterations = 3000\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    ppo_result = ppo_agent.train()\n",
    "    print(f\"Iteration {i + 1}:\")\n",
    "    print(f\"PPO: episode_reward_mean={ppo_result['episode_reward_mean']}\")\n",
    "    \n",
    "    # Record the reward\n",
    "    rewards.append(ppo_result['episode_reward_mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ppo_agent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m modelpath \u001b[39m=\u001b[39m ppo_agent\u001b[39m.\u001b[39msave(\u001b[39m'\u001b[39m\u001b[39m./model\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m os\u001b[39m.\u001b[39mmakedirs(modelpath\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m/fig\u001b[39m\u001b[39m'\u001b[39m, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m \u001b[39m# Plot rewards\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ppo_agent' is not defined"
     ]
    }
   ],
   "source": [
    "modelpath = ppo_agent.save('./model')\n",
    "os.makedirs(modelpath+'/fig', exist_ok=True)\n",
    "\n",
    "# Plot rewards\n",
    "plt.plot(rewards)\n",
    "plt.title(\"Rewards Over Iterations\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.savefig(modelpath+'/fig/reward_over_iterations.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-17 14:16:43,966\tINFO worker.py:1454 -- Calling ray.init() again after it has already been called.\n",
      "2023-05-17 14:16:46,941\tERROR actor_manager.py:507 -- Ray error, taking actor 1 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=36124, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x0000023E19943370>)\n",
      "  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\utils\\gym.py\", line 30, in check_old_gym_env\n",
      "    raise ValueError(\n",
      "ValueError: The number of values returned from `gym.Env.reset(seed=.., options=..)` must be 2! Make sure your `reset()` method returns: [obs] and [infos].\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=36124, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x0000023E19943370>)\n",
      "  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\utils\\pre_checks\\env.py\", line 81, in check_env\n",
      "    check_gym_environments(env)\n",
      "  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\utils\\pre_checks\\env.py\", line 184, in check_gym_environments\n",
      "    raise ValueError(\n",
      "ValueError: Your environment (<StockTradingEnv instance>) does not abide to the new gymnasium-style API!\n",
      "From Ray 2.3 on, RLlib only supports the new (gym>=0.26 or gymnasium) Env APIs.\n",
      "In particular, the `reset()` method seems to be faulty.\n",
      "Learn more about the most important changes here:\n",
      "https://github.com/openai/gym and here: https://github.com/Farama-Foundation/Gymnasium\n",
      "\n",
      "In order to fix this problem, do the following:\n",
      "\n",
      "1) Run `pip install gymnasium` on your command line.\n",
      "2) Change all your import statements in your code from\n",
      "   `import gym` -> `import gymnasium as gym` OR\n",
      "   `from gym.space import Discrete` -> `from gymnasium.spaces import Discrete`\n",
      "\n",
      "For your custom (single agent) gym.Env classes:\n",
      "3.1) Either wrap your old Env class via the provided `from gymnasium.wrappers import\n",
      "     EnvCompatibility` wrapper class.\n",
      "3.2) Alternatively to 3.1:\n",
      " - Change your `reset()` method to have the call signature 'def reset(self, *,\n",
      "   seed=None, options=None)'\n",
      " - Return an additional info dict (empty dict should be fine) from your `reset()`\n",
      "   method.\n",
      " - Return an additional `truncated` flag from your `step()` method (between `done` and\n",
      "   `info`). This flag should indicate, whether the episode was terminated prematurely\n",
      "   due to some time constraint or other kind of horizon setting.\n",
      "\n",
      "For your custom RLlib `MultiAgentEnv` classes:\n",
      "4.1) Either wrap your old MultiAgentEnv via the provided\n",
      "     `from ray.rllib.env.wrappers.multi_agent_env_compatibility import\n",
      "     MultiAgentEnvCompatibility` wrapper class.\n",
      "4.2) Alternatively to 4.1:\n",
      " - Change your `reset()` method to have the call signature\n",
      "   'def reset(self, *, seed=None, options=None)'\n",
      " - Return an additional per-agent info dict (empty dict should be fine) from your\n",
      "   `reset()` method.\n",
      " - Rename `dones` into `terminateds` and only set this to True, if the episode is really\n",
      "   done (as opposed to has been terminated prematurely due to some horizon/time-limit\n",
      "   setting).\n",
      " - Return an additional `truncateds` per-agent dictionary flag from your `step()`\n",
      "   method, including the `__all__` key (100% analogous to your `dones/terminateds`\n",
      "   per-agent dict).\n",
      "   Return this new `truncateds` dict between `dones/terminateds` and `infos`. This\n",
      "   flag should indicate, whether the episode (for some agent or all agents) was\n",
      "   terminated prematurely due to some time constraint or other kind of horizon setting.\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=36124, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x0000023E19943370>)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 870, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 921, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 877, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 881, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 821, in ray._raylet.execute_task.function_executor\n",
      "  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 670, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 460, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 616, in __init__\n",
      "    check_env(self.env)\n",
      "  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\utils\\pre_checks\\env.py\", line 92, in check_env\n",
      "    raise ValueError(\n",
      "ValueError: Traceback (most recent call last):\n",
      "  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\utils\\pre_checks\\env.py\", line 182, in check_gym_environments\n",
      "    check_old_gym_env(reset_results=obs_and_infos)\n",
      "  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\utils\\gym.py\", line 30, in check_old_gym_env\n",
      "    raise ValueError(\n",
      "ValueError: The number of values returned from `gym.Env.reset(seed=.., options=..)` must be 2! Make sure your `reset()` method returns: [obs] and [infos].\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=36124, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x0000023E19943370>)\n",
      "  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\utils\\pre_checks\\env.py\", line 81, in check_env\n",
      "    check_gym_environments(env)\n",
      "  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\utils\\pre_checks\\env.py\", line 184, in check_gym_environments\n",
      "    raise ValueError(\n",
      "ValueError: Your environment (<StockTradingEnv instance>) does not abide to the new gymnasium-style API!\n",
      "From Ray 2.3 on, RLlib only supports the new (gym>=0.26 or gymnasium) Env APIs.\n",
      "In particular, the `reset()` method seems to be faulty.\n",
      "Learn more about the most important changes here:\n",
      "https://github.com/openai/gym and here: https://github.com/Farama-Foundation/Gymnasium\n",
      "\n",
      "In order to fix this problem, do the following:\n",
      "\n",
      "1) Run `pip install gymnasium` on your command line.\n",
      "2) Change all your import statements in your code from\n",
      "   `import gym` -> `import gymnasium as gym` OR\n",
      "   `from gym.space import Discrete` -> `from gymnasium.spaces import Discrete`\n",
      "\n",
      "For your custom (single agent) gym.Env classes:\n",
      "3.1) Either wrap your old Env class via the provided `from gymnasium.wrappers import\n",
      "     EnvCompatibility` wrapper class.\n",
      "3.2) Alternatively to 3.1:\n",
      " - Change your `reset()` method to have the call signature 'def reset(self, *,\n",
      "   seed=None, options=None)'\n",
      " - Return an additional info dict (empty dict should be fine) from your `reset()`\n",
      "   method.\n",
      " - Return an additional `truncated` flag from your `step()` method (between `done` and\n",
      "   `info`). This flag should indicate, whether the episode was terminated prematurely\n",
      "   due to some time constraint or other kind of horizon setting.\n",
      "\n",
      "For your custom RLlib `MultiAgentEnv` classes:\n",
      "4.1) Either wrap your old MultiAgentEnv via the provided\n",
      "     `from ray.rllib.env.wrappers.multi_agent_env_compatibility import\n",
      "     MultiAgentEnvCompatibility` wrapper class.\n",
      "4.2) Alternatively to 4.1:\n",
      " - Change your `reset()` method to have the call signature\n",
      "   'def reset(self, *, seed=None, options=None)'\n",
      " - Return an additional per-agent info dict (empty dict should be fine) from your\n",
      "   `reset()` method.\n",
      " - Rename `dones` into `terminateds` and only set this to True, if the episode is really\n",
      "   done (as opposed to has been terminated prematurely due to some horizon/time-limit\n",
      "   setting).\n",
      " - Return an additional `truncateds` per-agent dictionary flag from your `step()`\n",
      "   method, including the `__all__` key (100% analogous to your `dones/terminateds`\n",
      "   per-agent dict).\n",
      "   Return this new `truncateds` dict between `dones/terminateds` and `infos`. This\n",
      "   flag should indicate, whether the episode (for some agent or all agents) was\n",
      "   terminated prematurely due to some time constraint or other kind of horizon setting.\n",
      "\n",
      "\n",
      "The above error has been found in your environment! We've added a module for checking your custom environments. It may cause your experiment to fail if your environment is not set up correctly. You can disable this behavior via calling `config.environment(disable_env_checking=True)`. You can run the environment checking module standalone by calling ray.rllib.utils.check_env([your env]).\n",
      "2023-05-17 14:16:46,942\tERROR actor_manager.py:507 -- Ray error, taking actor 2 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=34776, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x0000028F2B393370>)\n",
      "  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\utils\\gym.py\", line 30, in check_old_gym_env\n",
      "    raise ValueError(\n",
      "ValueError: The number of values returned from `gym.Env.reset(seed=.., options=..)` must be 2! Make sure your `reset()` method returns: [obs] and [infos].\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=34776, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x0000028F2B393370>)\n",
      "  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\utils\\pre_checks\\env.py\", line 81, in check_env\n",
      "    check_gym_environments(env)\n",
      "  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\utils\\pre_checks\\env.py\", line 184, in check_gym_environments\n",
      "    raise ValueError(\n",
      "ValueError: Your environment (<StockTradingEnv instance>) does not abide to the new gymnasium-style API!\n",
      "From Ray 2.3 on, RLlib only supports the new (gym>=0.26 or gymnasium) Env APIs.\n",
      "In particular, the `reset()` method seems to be faulty.\n",
      "Learn more about the most important changes here:\n",
      "https://github.com/openai/gym and here: https://github.com/Farama-Foundation/Gymnasium\n",
      "\n",
      "In order to fix this problem, do the following:\n",
      "\n",
      "1) Run `pip install gymnasium` on your command line.\n",
      "2) Change all your import statements in your code from\n",
      "   `import gym` -> `import gymnasium as gym` OR\n",
      "   `from gym.space import Discrete` -> `from gymnasium.spaces import Discrete`\n",
      "\n",
      "For your custom (single agent) gym.Env classes:\n",
      "3.1) Either wrap your old Env class via the provided `from gymnasium.wrappers import\n",
      "     EnvCompatibility` wrapper class.\n",
      "3.2) Alternatively to 3.1:\n",
      " - Change your `reset()` method to have the call signature 'def reset(self, *,\n",
      "   seed=None, options=None)'\n",
      " - Return an additional info dict (empty dict should be fine) from your `reset()`\n",
      "   method.\n",
      " - Return an additional `truncated` flag from your `step()` method (between `done` and\n",
      "   `info`). This flag should indicate, whether the episode was terminated prematurely\n",
      "   due to some time constraint or other kind of horizon setting.\n",
      "\n",
      "For your custom RLlib `MultiAgentEnv` classes:\n",
      "4.1) Either wrap your old MultiAgentEnv via the provided\n",
      "     `from ray.rllib.env.wrappers.multi_agent_env_compatibility import\n",
      "     MultiAgentEnvCompatibility` wrapper class.\n",
      "4.2) Alternatively to 4.1:\n",
      " - Change your `reset()` method to have the call signature\n",
      "   'def reset(self, *, seed=None, options=None)'\n",
      " - Return an additional per-agent info dict (empty dict should be fine) from your\n",
      "   `reset()` method.\n",
      " - Rename `dones` into `terminateds` and only set this to True, if the episode is really\n",
      "   done (as opposed to has been terminated prematurely due to some horizon/time-limit\n",
      "   setting).\n",
      " - Return an additional `truncateds` per-agent dictionary flag from your `step()`\n",
      "   method, including the `__all__` key (100% analogous to your `dones/terminateds`\n",
      "   per-agent dict).\n",
      "   Return this new `truncateds` dict between `dones/terminateds` and `infos`. This\n",
      "   flag should indicate, whether the episode (for some agent or all agents) was\n",
      "   terminated prematurely due to some time constraint or other kind of horizon setting.\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=34776, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x0000028F2B393370>)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 870, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 921, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 877, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 881, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 821, in ray._raylet.execute_task.function_executor\n",
      "  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 670, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 460, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 616, in __init__\n",
      "    check_env(self.env)\n",
      "  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\utils\\pre_checks\\env.py\", line 92, in check_env\n",
      "    raise ValueError(\n",
      "ValueError: Traceback (most recent call last):\n",
      "  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\utils\\pre_checks\\env.py\", line 182, in check_gym_environments\n",
      "    check_old_gym_env(reset_results=obs_and_infos)\n",
      "  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\utils\\gym.py\", line 30, in check_old_gym_env\n",
      "    raise ValueError(\n",
      "ValueError: The number of values returned from `gym.Env.reset(seed=.., options=..)` must be 2! Make sure your `reset()` method returns: [obs] and [infos].\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=34776, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x0000028F2B393370>)\n",
      "  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\utils\\pre_checks\\env.py\", line 81, in check_env\n",
      "    check_gym_environments(env)\n",
      "  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\utils\\pre_checks\\env.py\", line 184, in check_gym_environments\n",
      "    raise ValueError(\n",
      "ValueError: Your environment (<StockTradingEnv instance>) does not abide to the new gymnasium-style API!\n",
      "From Ray 2.3 on, RLlib only supports the new (gym>=0.26 or gymnasium) Env APIs.\n",
      "In particular, the `reset()` method seems to be faulty.\n",
      "Learn more about the most important changes here:\n",
      "https://github.com/openai/gym and here: https://github.com/Farama-Foundation/Gymnasium\n",
      "\n",
      "In order to fix this problem, do the following:\n",
      "\n",
      "1) Run `pip install gymnasium` on your command line.\n",
      "2) Change all your import statements in your code from\n",
      "   `import gym` -> `import gymnasium as gym` OR\n",
      "   `from gym.space import Discrete` -> `from gymnasium.spaces import Discrete`\n",
      "\n",
      "For your custom (single agent) gym.Env classes:\n",
      "3.1) Either wrap your old Env class via the provided `from gymnasium.wrappers import\n",
      "     EnvCompatibility` wrapper class.\n",
      "3.2) Alternatively to 3.1:\n",
      " - Change your `reset()` method to have the call signature 'def reset(self, *,\n",
      "   seed=None, options=None)'\n",
      " - Return an additional info dict (empty dict should be fine) from your `reset()`\n",
      "   method.\n",
      " - Return an additional `truncated` flag from your `step()` method (between `done` and\n",
      "   `info`). This flag should indicate, whether the episode was terminated prematurely\n",
      "   due to some time constraint or other kind of horizon setting.\n",
      "\n",
      "For your custom RLlib `MultiAgentEnv` classes:\n",
      "4.1) Either wrap your old MultiAgentEnv via the provided\n",
      "     `from ray.rllib.env.wrappers.multi_agent_env_compatibility import\n",
      "     MultiAgentEnvCompatibility` wrapper class.\n",
      "4.2) Alternatively to 4.1:\n",
      " - Change your `reset()` method to have the call signature\n",
      "   'def reset(self, *, seed=None, options=None)'\n",
      " - Return an additional per-agent info dict (empty dict should be fine) from your\n",
      "   `reset()` method.\n",
      " - Rename `dones` into `terminateds` and only set this to True, if the episode is really\n",
      "   done (as opposed to has been terminated prematurely due to some horizon/time-limit\n",
      "   setting).\n",
      " - Return an additional `truncateds` per-agent dictionary flag from your `step()`\n",
      "   method, including the `__all__` key (100% analogous to your `dones/terminateds`\n",
      "   per-agent dict).\n",
      "   Return this new `truncateds` dict between `dones/terminateds` and `infos`. This\n",
      "   flag should indicate, whether the episode (for some agent or all agents) was\n",
      "   terminated prematurely due to some time constraint or other kind of horizon setting.\n",
      "\n",
      "\n",
      "The above error has been found in your environment! We've added a module for checking your custom environments. It may cause your experiment to fail if your environment is not set up correctly. You can disable this behavior via calling `config.environment(disable_env_checking=True)`. You can run the environment checking module standalone by calling ray.rllib.utils.check_env([your env]).\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34776)\u001b[0m 2023-05-17 14:16:46,935\tERROR worker.py:844 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=34776, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x0000028F2B393370>)\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34776)\u001b[0m   File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\utils\\gym.py\", line 30, in check_old_gym_env\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34776)\u001b[0m     raise ValueError(\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34776)\u001b[0m ValueError: The number of values returned from `gym.Env.reset(seed=.., options=..)` must be 2! Make sure your `reset()` method returns: [obs] and [infos].\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34776)\u001b[0m \u001b[32m [repeated 34x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34776)\u001b[0m The above exception was the direct cause of the following exception:\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34776)\u001b[0m \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=34776, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x0000028F2B393370>)\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34776)\u001b[0m   File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\utils\\pre_checks\\env.py\", line 81, in check_env\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34776)\u001b[0m     check_gym_environments(env)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34776)\u001b[0m   File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\utils\\pre_checks\\env.py\", line 184, in check_gym_environments\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34776)\u001b[0m ValueError: Your environment (<StockTradingEnv instance>) does not abide to the new gymnasium-style API!\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34776)\u001b[0m From Ray 2.3 on, RLlib only supports the new (gym>=0.26 or gymnasium) Env APIs.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34776)\u001b[0m In particular, the `reset()` method seems to be faulty.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34776)\u001b[0m Learn more about the most important changes here:\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34776)\u001b[0m https://github.com/openai/gym and here: https://github.com/Farama-Foundation/Gymnasium\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34776)\u001b[0m In order to fix this problem, do the following:\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34776)\u001b[0m 1) Run `pip install gymnasium` on your command line.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34776)\u001b[0m 2) Change all your import statements in your code from\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34776)\u001b[0m    `import gym` -> `import gymnasium as gym` OR\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34776)\u001b[0m    `from gym.space import Discrete` -> `from gymnasium.spaces import Discrete`\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34776)\u001b[0m For your custom (single agent) gym.Env classes:\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34776)\u001b[0m 3.1) Either wrap your old Env class via the provided `from gymnasium.wrappers import\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34776)\u001b[0m      EnvCompatibility` wrapper class.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34776)\u001b[0m 4.2) Alternatively to 4.1:\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34776)\u001b[0m  - Change your `reset()` method to have the call signature 'def reset(self, *,\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34776)\u001b[0m    seed=None, options=None)'\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34776)\u001b[0m  - Return an additional info dict (empty dict should be fine) from your `reset()`\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34776)\u001b[0m    method.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34776)\u001b[0m  - Return an additional `truncated` flag from your `step()` method (between `done` and\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34776)\u001b[0m    `info`). This flag should indicate, whether the episode was terminated prematurely\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34776)\u001b[0m    due to some time constraint or other kind of horizon setting.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34776)\u001b[0m For your custom RLlib `MultiAgentEnv` classes:\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34776)\u001b[0m 4.1) Either wrap your old MultiAgentEnv via the provided\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34776)\u001b[0m      `from ray.rllib.env.wrappers.multi_agent_env_compatibility import\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34776)\u001b[0m      MultiAgentEnvCompatibility` wrapper class.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34776)\u001b[0m  - Change your `reset()` method to have the call signature\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34776)\u001b[0m    'def reset(self, *, seed=None, options=None)'\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34776)\u001b[0m  - Return an additional per-agent info dict (empty dict should be fine) from your\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34776)\u001b[0m    `reset()` method.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34776)\u001b[0m  - Rename `dones` into `terminateds` and only set this to True, if the episode is really\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34776)\u001b[0m    done (as opposed to has been terminated prematurely due to some horizon/time-limit\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34776)\u001b[0m    setting).\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34776)\u001b[0m  - Return an additional `truncateds` per-agent dictionary flag from your `step()`\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34776)\u001b[0m    method, including the `__all__` key (100% analogous to your `dones/terminateds`\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34776)\u001b[0m    per-agent dict).\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34776)\u001b[0m    Return this new `truncateds` dict between `dones/terminateds` and `infos`. This\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34776)\u001b[0m    flag should indicate, whether the episode (for some agent or all agents) was\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34776)\u001b[0m    terminated prematurely due to some time constraint or other kind of horizon setting.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34776)\u001b[0m During handling of the above exception, another exception occurred:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34776)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 881, in ray._raylet.execute_task\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34776)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 821, in ray._raylet.execute_task.function_executor\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34776)\u001b[0m   File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 670, in actor_method_executor\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34776)\u001b[0m     return method(__ray_actor, *args, **kwargs)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34776)\u001b[0m   File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 460, in _resume_span\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34776)\u001b[0m     return method(self, *_args, **_kwargs)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34776)\u001b[0m   File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 616, in __init__\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34776)\u001b[0m     check_env(self.env)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34776)\u001b[0m ValueError: Traceback (most recent call last):\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34776)\u001b[0m     check_old_gym_env(reset_results=obs_and_infos)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34776)\u001b[0m The above error has been found in your environment! We've added a module for checking your custom environments. It may cause your experiment to fail if your environment is not set up correctly. You can disable this behavior via calling `config.environment(disable_env_checking=True)`. You can run the environment checking module standalone by calling ray.rllib.utils.check_env([your env]).\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=36124)\u001b[0m 2023-05-17 14:16:46,897\tWARNING env.py:155 -- Your env doesn't have a .spec.max_episode_steps attribute. Your horizon will default to infinity, and your environment will not be reset.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Traceback (most recent call last):\n  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\utils\\pre_checks\\env.py\", line 182, in check_gym_environments\n    check_old_gym_env(reset_results=obs_and_infos)\n  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\utils\\gym.py\", line 30, in check_old_gym_env\n    raise ValueError(\nValueError: The number of values returned from `gym.Env.reset(seed=.., options=..)` must be 2! Make sure your `reset()` method returns: [obs] and [infos].\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\utils\\pre_checks\\env.py\", line 81, in check_env\n    check_gym_environments(env)\n  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\utils\\pre_checks\\env.py\", line 184, in check_gym_environments\n    raise ValueError(\nValueError: Your environment (<StockTradingEnv instance>) does not abide to the new gymnasium-style API!\nFrom Ray 2.3 on, RLlib only supports the new (gym>=0.26 or gymnasium) Env APIs.\nIn particular, the `reset()` method seems to be faulty.\nLearn more about the most important changes here:\nhttps://github.com/openai/gym and here: https://github.com/Farama-Foundation/Gymnasium\n\nIn order to fix this problem, do the following:\n\n1) Run `pip install gymnasium` on your command line.\n2) Change all your import statements in your code from\n   `import gym` -> `import gymnasium as gym` OR\n   `from gym.space import Discrete` -> `from gymnasium.spaces import Discrete`\n\nFor your custom (single agent) gym.Env classes:\n3.1) Either wrap your old Env class via the provided `from gymnasium.wrappers import\n     EnvCompatibility` wrapper class.\n3.2) Alternatively to 3.1:\n - Change your `reset()` method to have the call signature 'def reset(self, *,\n   seed=None, options=None)'\n - Return an additional info dict (empty dict should be fine) from your `reset()`\n   method.\n - Return an additional `truncated` flag from your `step()` method (between `done` and\n   `info`). This flag should indicate, whether the episode was terminated prematurely\n   due to some time constraint or other kind of horizon setting.\n\nFor your custom RLlib `MultiAgentEnv` classes:\n4.1) Either wrap your old MultiAgentEnv via the provided\n     `from ray.rllib.env.wrappers.multi_agent_env_compatibility import\n     MultiAgentEnvCompatibility` wrapper class.\n4.2) Alternatively to 4.1:\n - Change your `reset()` method to have the call signature\n   'def reset(self, *, seed=None, options=None)'\n - Return an additional per-agent info dict (empty dict should be fine) from your\n   `reset()` method.\n - Rename `dones` into `terminateds` and only set this to True, if the episode is really\n   done (as opposed to has been terminated prematurely due to some horizon/time-limit\n   setting).\n - Return an additional `truncateds` per-agent dictionary flag from your `step()`\n   method, including the `__all__` key (100% analogous to your `dones/terminateds`\n   per-agent dict).\n   Return this new `truncateds` dict between `dones/terminateds` and `infos`. This\n   flag should indicate, whether the episode (for some agent or all agents) was\n   terminated prematurely due to some time constraint or other kind of horizon setting.\n\n\nThe above error has been found in your environment! We've added a module for checking your custom environments. It may cause your experiment to fail if your environment is not set up correctly. You can disable this behavior via calling `config.environment(disable_env_checking=True)`. You can run the environment checking module standalone by calling ray.rllib.utils.check_env([your env]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRayActorError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\evaluation\\worker_set.py:172\u001b[0m, in \u001b[0;36mWorkerSet.__init__\u001b[1;34m(self, env_creator, validate_env, default_policy_class, config, num_workers, local_worker, logdir, _setup, policy_class, trainer_config)\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 172\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_setup(\n\u001b[0;32m    173\u001b[0m         validate_env\u001b[39m=\u001b[39;49mvalidate_env,\n\u001b[0;32m    174\u001b[0m         config\u001b[39m=\u001b[39;49mconfig,\n\u001b[0;32m    175\u001b[0m         num_workers\u001b[39m=\u001b[39;49mnum_workers,\n\u001b[0;32m    176\u001b[0m         local_worker\u001b[39m=\u001b[39;49mlocal_worker,\n\u001b[0;32m    177\u001b[0m     )\n\u001b[0;32m    178\u001b[0m \u001b[39m# WorkerSet creation possibly fails, if some (remote) workers cannot\u001b[39;00m\n\u001b[0;32m    179\u001b[0m \u001b[39m# be initialized properly (due to some errors in the RolloutWorker's\u001b[39;00m\n\u001b[0;32m    180\u001b[0m \u001b[39m# constructor).\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\evaluation\\worker_set.py:242\u001b[0m, in \u001b[0;36mWorkerSet._setup\u001b[1;34m(self, validate_env, config, num_workers, local_worker)\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[39m# Create a number of @ray.remote workers.\u001b[39;00m\n\u001b[1;32m--> 242\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_workers(\n\u001b[0;32m    243\u001b[0m     num_workers,\n\u001b[0;32m    244\u001b[0m     validate\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mvalidate_workers_after_construction,\n\u001b[0;32m    245\u001b[0m )\n\u001b[0;32m    247\u001b[0m \u001b[39m# If num_workers > 0 and we don't have an env on the local worker,\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \u001b[39m# get the observation- and action spaces for each policy from\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[39m# the first remote worker (which does have an env).\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\evaluation\\worker_set.py:635\u001b[0m, in \u001b[0;36mWorkerSet.add_workers\u001b[1;34m(self, num_workers, validate)\u001b[0m\n\u001b[0;32m    634\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m result\u001b[39m.\u001b[39mok:\n\u001b[1;32m--> 635\u001b[0m     \u001b[39mraise\u001b[39;00m result\u001b[39m.\u001b[39mget()\n",
      "File \u001b[1;32mc:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\utils\\actor_manager.py:488\u001b[0m, in \u001b[0;36mFaultTolerantActorManager.__fetch_result\u001b[1;34m(self, remote_actor_ids, remote_calls, tags, timeout_seconds, return_obj_refs, mark_healthy)\u001b[0m\n\u001b[0;32m    487\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 488\u001b[0m     result \u001b[39m=\u001b[39m ray\u001b[39m.\u001b[39;49mget(r)\n\u001b[0;32m    489\u001b[0m     remote_results\u001b[39m.\u001b[39madd_result(actor_id, ResultOrError(result\u001b[39m=\u001b[39mresult), tag)\n",
      "File \u001b[1;32mc:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\_private\\client_mode_hook.py:105\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(ray, func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 105\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\_private\\worker.py:2523\u001b[0m, in \u001b[0;36mget\u001b[1;34m(object_refs, timeout)\u001b[0m\n\u001b[0;32m   2522\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2523\u001b[0m             \u001b[39mraise\u001b[39;00m value\n\u001b[0;32m   2525\u001b[0m \u001b[39mif\u001b[39;00m is_individual_id:\n",
      "\u001b[1;31mRayActorError\u001b[0m: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=36124, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x0000023E19943370>)\n  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\utils\\gym.py\", line 30, in check_old_gym_env\n    raise ValueError(\nValueError: The number of values returned from `gym.Env.reset(seed=.., options=..)` must be 2! Make sure your `reset()` method returns: [obs] and [infos].\n\nThe above exception was the direct cause of the following exception:\n\n\u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=36124, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x0000023E19943370>)\n  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\utils\\pre_checks\\env.py\", line 81, in check_env\n    check_gym_environments(env)\n  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\utils\\pre_checks\\env.py\", line 184, in check_gym_environments\n    raise ValueError(\nValueError: Your environment (<StockTradingEnv instance>) does not abide to the new gymnasium-style API!\nFrom Ray 2.3 on, RLlib only supports the new (gym>=0.26 or gymnasium) Env APIs.\nIn particular, the `reset()` method seems to be faulty.\nLearn more about the most important changes here:\nhttps://github.com/openai/gym and here: https://github.com/Farama-Foundation/Gymnasium\n\nIn order to fix this problem, do the following:\n\n1) Run `pip install gymnasium` on your command line.\n2) Change all your import statements in your code from\n   `import gym` -> `import gymnasium as gym` OR\n   `from gym.space import Discrete` -> `from gymnasium.spaces import Discrete`\n\nFor your custom (single agent) gym.Env classes:\n3.1) Either wrap your old Env class via the provided `from gymnasium.wrappers import\n     EnvCompatibility` wrapper class.\n3.2) Alternatively to 3.1:\n - Change your `reset()` method to have the call signature 'def reset(self, *,\n   seed=None, options=None)'\n - Return an additional info dict (empty dict should be fine) from your `reset()`\n   method.\n - Return an additional `truncated` flag from your `step()` method (between `done` and\n   `info`). This flag should indicate, whether the episode was terminated prematurely\n   due to some time constraint or other kind of horizon setting.\n\nFor your custom RLlib `MultiAgentEnv` classes:\n4.1) Either wrap your old MultiAgentEnv via the provided\n     `from ray.rllib.env.wrappers.multi_agent_env_compatibility import\n     MultiAgentEnvCompatibility` wrapper class.\n4.2) Alternatively to 4.1:\n - Change your `reset()` method to have the call signature\n   'def reset(self, *, seed=None, options=None)'\n - Return an additional per-agent info dict (empty dict should be fine) from your\n   `reset()` method.\n - Rename `dones` into `terminateds` and only set this to True, if the episode is really\n   done (as opposed to has been terminated prematurely due to some horizon/time-limit\n   setting).\n - Return an additional `truncateds` per-agent dictionary flag from your `step()`\n   method, including the `__all__` key (100% analogous to your `dones/terminateds`\n   per-agent dict).\n   Return this new `truncateds` dict between `dones/terminateds` and `infos`. This\n   flag should indicate, whether the episode (for some agent or all agents) was\n   terminated prematurely due to some time constraint or other kind of horizon setting.\n\n\nDuring handling of the above exception, another exception occurred:\n\n\u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=36124, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x0000023E19943370>)\n  File \"python\\ray\\_raylet.pyx\", line 870, in ray._raylet.execute_task\n  File \"python\\ray\\_raylet.pyx\", line 921, in ray._raylet.execute_task\n  File \"python\\ray\\_raylet.pyx\", line 877, in ray._raylet.execute_task\n  File \"python\\ray\\_raylet.pyx\", line 881, in ray._raylet.execute_task\n  File \"python\\ray\\_raylet.pyx\", line 821, in ray._raylet.execute_task.function_executor\n  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 670, in actor_method_executor\n    return method(__ray_actor, *args, **kwargs)\n  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 460, in _resume_span\n    return method(self, *_args, **_kwargs)\n  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 616, in __init__\n    check_env(self.env)\n  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\utils\\pre_checks\\env.py\", line 92, in check_env\n    raise ValueError(\nValueError: Traceback (most recent call last):\n  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\utils\\pre_checks\\env.py\", line 182, in check_gym_environments\n    check_old_gym_env(reset_results=obs_and_infos)\n  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\utils\\gym.py\", line 30, in check_old_gym_env\n    raise ValueError(\nValueError: The number of values returned from `gym.Env.reset(seed=.., options=..)` must be 2! Make sure your `reset()` method returns: [obs] and [infos].\n\nThe above exception was the direct cause of the following exception:\n\n\u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=36124, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x0000023E19943370>)\n  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\utils\\pre_checks\\env.py\", line 81, in check_env\n    check_gym_environments(env)\n  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\utils\\pre_checks\\env.py\", line 184, in check_gym_environments\n    raise ValueError(\nValueError: Your environment (<StockTradingEnv instance>) does not abide to the new gymnasium-style API!\nFrom Ray 2.3 on, RLlib only supports the new (gym>=0.26 or gymnasium) Env APIs.\nIn particular, the `reset()` method seems to be faulty.\nLearn more about the most important changes here:\nhttps://github.com/openai/gym and here: https://github.com/Farama-Foundation/Gymnasium\n\nIn order to fix this problem, do the following:\n\n1) Run `pip install gymnasium` on your command line.\n2) Change all your import statements in your code from\n   `import gym` -> `import gymnasium as gym` OR\n   `from gym.space import Discrete` -> `from gymnasium.spaces import Discrete`\n\nFor your custom (single agent) gym.Env classes:\n3.1) Either wrap your old Env class via the provided `from gymnasium.wrappers import\n     EnvCompatibility` wrapper class.\n3.2) Alternatively to 3.1:\n - Change your `reset()` method to have the call signature 'def reset(self, *,\n   seed=None, options=None)'\n - Return an additional info dict (empty dict should be fine) from your `reset()`\n   method.\n - Return an additional `truncated` flag from your `step()` method (between `done` and\n   `info`). This flag should indicate, whether the episode was terminated prematurely\n   due to some time constraint or other kind of horizon setting.\n\nFor your custom RLlib `MultiAgentEnv` classes:\n4.1) Either wrap your old MultiAgentEnv via the provided\n     `from ray.rllib.env.wrappers.multi_agent_env_compatibility import\n     MultiAgentEnvCompatibility` wrapper class.\n4.2) Alternatively to 4.1:\n - Change your `reset()` method to have the call signature\n   'def reset(self, *, seed=None, options=None)'\n - Return an additional per-agent info dict (empty dict should be fine) from your\n   `reset()` method.\n - Rename `dones` into `terminateds` and only set this to True, if the episode is really\n   done (as opposed to has been terminated prematurely due to some horizon/time-limit\n   setting).\n - Return an additional `truncateds` per-agent dictionary flag from your `step()`\n   method, including the `__all__` key (100% analogous to your `dones/terminateds`\n   per-agent dict).\n   Return this new `truncateds` dict between `dones/terminateds` and `infos`. This\n   flag should indicate, whether the episode (for some agent or all agents) was\n   terminated prematurely due to some time constraint or other kind of horizon setting.\n\n\nThe above error has been found in your environment! We've added a module for checking your custom environments. It may cause your experiment to fail if your environment is not set up correctly. You can disable this behavior via calling `config.environment(disable_env_checking=True)`. You can run the environment checking module standalone by calling ray.rllib.utils.check_env([your env]).",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 127\u001b[0m\n\u001b[0;32m    125\u001b[0m ppo_config[\u001b[39m\"\u001b[39m\u001b[39mseed\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m42\u001b[39m\n\u001b[0;32m    126\u001b[0m ppo_agent \u001b[39m=\u001b[39m ppo_config\u001b[39m.\u001b[39menvironment(env\u001b[39m=\u001b[39menv_name)\n\u001b[1;32m--> 127\u001b[0m ppo_agent \u001b[39m=\u001b[39m ppo_config\u001b[39m.\u001b[39;49mbuild()\n\u001b[0;32m    128\u001b[0m ppo_agent\u001b[39m.\u001b[39mrestore(modelpath)\n\u001b[0;32m    130\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mEvaluating trained agents:\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm_config.py:1071\u001b[0m, in \u001b[0;36mAlgorithmConfig.build\u001b[1;34m(self, env, logger_creator, use_copy)\u001b[0m\n\u001b[0;32m   1068\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39malgo_class, \u001b[39mstr\u001b[39m):\n\u001b[0;32m   1069\u001b[0m     algo_class \u001b[39m=\u001b[39m get_trainable_cls(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39malgo_class)\n\u001b[1;32m-> 1071\u001b[0m \u001b[39mreturn\u001b[39;00m algo_class(\n\u001b[0;32m   1072\u001b[0m     config\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m use_copy \u001b[39melse\u001b[39;49;00m copy\u001b[39m.\u001b[39;49mdeepcopy(\u001b[39mself\u001b[39;49m),\n\u001b[0;32m   1073\u001b[0m     logger_creator\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlogger_creator,\n\u001b[0;32m   1074\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py:466\u001b[0m, in \u001b[0;36mAlgorithm.__init__\u001b[1;34m(self, config, env, logger_creator, **kwargs)\u001b[0m\n\u001b[0;32m    454\u001b[0m \u001b[39m# Initialize common evaluation_metrics to nan, before they become\u001b[39;00m\n\u001b[0;32m    455\u001b[0m \u001b[39m# available. We want to make sure the metrics are always present\u001b[39;00m\n\u001b[0;32m    456\u001b[0m \u001b[39m# (although their values may be nan), so that Tune does not complain\u001b[39;00m\n\u001b[0;32m    457\u001b[0m \u001b[39m# when we use these as stopping criteria.\u001b[39;00m\n\u001b[0;32m    458\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation_metrics \u001b[39m=\u001b[39m {\n\u001b[0;32m    459\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mevaluation\u001b[39m\u001b[39m\"\u001b[39m: {\n\u001b[0;32m    460\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mepisode_reward_max\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mnan,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    463\u001b[0m     }\n\u001b[0;32m    464\u001b[0m }\n\u001b[1;32m--> 466\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\n\u001b[0;32m    467\u001b[0m     config\u001b[39m=\u001b[39mconfig,\n\u001b[0;32m    468\u001b[0m     logger_creator\u001b[39m=\u001b[39mlogger_creator,\n\u001b[0;32m    469\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    470\u001b[0m )\n\u001b[0;32m    472\u001b[0m \u001b[39m# Check, whether `training_iteration` is still a tune.Trainable property\u001b[39;00m\n\u001b[0;32m    473\u001b[0m \u001b[39m# and has not been overridden by the user in the attempt to implement the\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[39m# algos logic (this should be done now inside `training_step`).\u001b[39;00m\n\u001b[0;32m    475\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\tune\\trainable\\trainable.py:169\u001b[0m, in \u001b[0;36mTrainable.__init__\u001b[1;34m(self, config, logger_creator, remote_checkpoint_dir, custom_syncer, sync_timeout, sync_config)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m    168\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_local_ip \u001b[39m=\u001b[39m ray\u001b[39m.\u001b[39mutil\u001b[39m.\u001b[39mget_node_ip_address()\n\u001b[1;32m--> 169\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msetup(copy\u001b[39m.\u001b[39;49mdeepcopy(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig))\n\u001b[0;32m    170\u001b[0m setup_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time\n\u001b[0;32m    171\u001b[0m \u001b[39mif\u001b[39;00m setup_time \u001b[39m>\u001b[39m SETUP_TIME_THRESHOLD:\n",
      "File \u001b[1;32mc:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py:592\u001b[0m, in \u001b[0;36mAlgorithm.setup\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[39m# Only if user did not override `_init()`:\u001b[39;00m\n\u001b[0;32m    586\u001b[0m \u001b[39mif\u001b[39;00m _init \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[0;32m    587\u001b[0m     \u001b[39m# - Create rollout workers here automatically.\u001b[39;00m\n\u001b[0;32m    588\u001b[0m     \u001b[39m# - Run the execution plan to create the local iterator to `next()`\u001b[39;00m\n\u001b[0;32m    589\u001b[0m     \u001b[39m#   in each training iteration.\u001b[39;00m\n\u001b[0;32m    590\u001b[0m     \u001b[39m# This matches the behavior of using `build_trainer()`, which\u001b[39;00m\n\u001b[0;32m    591\u001b[0m     \u001b[39m# has been deprecated.\u001b[39;00m\n\u001b[1;32m--> 592\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mworkers \u001b[39m=\u001b[39m WorkerSet(\n\u001b[0;32m    593\u001b[0m         env_creator\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv_creator,\n\u001b[0;32m    594\u001b[0m         validate_env\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalidate_env,\n\u001b[0;32m    595\u001b[0m         default_policy_class\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_default_policy_class(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig),\n\u001b[0;32m    596\u001b[0m         config\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig,\n\u001b[0;32m    597\u001b[0m         num_workers\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49mnum_rollout_workers,\n\u001b[0;32m    598\u001b[0m         local_worker\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    599\u001b[0m         logdir\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlogdir,\n\u001b[0;32m    600\u001b[0m     )\n\u001b[0;32m    602\u001b[0m     \u001b[39m# TODO (avnishn): Remove the execution plan API by q1 2023\u001b[39;00m\n\u001b[0;32m    603\u001b[0m     \u001b[39m# Function defining one single training iteration's behavior.\u001b[39;00m\n\u001b[0;32m    604\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39m_disable_execution_plan_api:\n\u001b[0;32m    605\u001b[0m         \u001b[39m# Ensure remote workers are initially in sync with the local worker.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\evaluation\\worker_set.py:194\u001b[0m, in \u001b[0;36mWorkerSet.__init__\u001b[1;34m(self, env_creator, validate_env, default_policy_class, config, num_workers, local_worker, logdir, _setup, policy_class, trainer_config)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[39mexcept\u001b[39;00m RayActorError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    182\u001b[0m     \u001b[39m# In case of an actor (remote worker) init failure, the remote worker\u001b[39;00m\n\u001b[0;32m    183\u001b[0m     \u001b[39m# may still exist and will be accessible, however, e.g. calling\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[39m# its `sample.remote()` would result in strange \"property not found\"\u001b[39;00m\n\u001b[0;32m    185\u001b[0m     \u001b[39m# errors.\u001b[39;00m\n\u001b[0;32m    186\u001b[0m     \u001b[39mif\u001b[39;00m e\u001b[39m.\u001b[39mactor_init_failed:\n\u001b[0;32m    187\u001b[0m         \u001b[39m# Raise the original error here that the RolloutWorker raised\u001b[39;00m\n\u001b[0;32m    188\u001b[0m         \u001b[39m# during its construction process. This is to enforce transparency\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    192\u001b[0m         \u001b[39m# - e.args[0].args[2]: The original Exception (e.g. a ValueError due\u001b[39;00m\n\u001b[0;32m    193\u001b[0m         \u001b[39m# to a config mismatch) thrown inside the actor.\u001b[39;00m\n\u001b[1;32m--> 194\u001b[0m         \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39margs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39margs[\u001b[39m2\u001b[39m]\n\u001b[0;32m    195\u001b[0m     \u001b[39m# In any other case, raise the RayActorError as-is.\u001b[39;00m\n\u001b[0;32m    196\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    197\u001b[0m         \u001b[39mraise\u001b[39;00m e\n",
      "\u001b[1;31mValueError\u001b[0m: Traceback (most recent call last):\n  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\utils\\pre_checks\\env.py\", line 182, in check_gym_environments\n    check_old_gym_env(reset_results=obs_and_infos)\n  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\utils\\gym.py\", line 30, in check_old_gym_env\n    raise ValueError(\nValueError: The number of values returned from `gym.Env.reset(seed=.., options=..)` must be 2! Make sure your `reset()` method returns: [obs] and [infos].\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\utils\\pre_checks\\env.py\", line 81, in check_env\n    check_gym_environments(env)\n  File \"c:\\Users\\rodin\\work\\stock_trade\\.venv\\lib\\site-packages\\ray\\rllib\\utils\\pre_checks\\env.py\", line 184, in check_gym_environments\n    raise ValueError(\nValueError: Your environment (<StockTradingEnv instance>) does not abide to the new gymnasium-style API!\nFrom Ray 2.3 on, RLlib only supports the new (gym>=0.26 or gymnasium) Env APIs.\nIn particular, the `reset()` method seems to be faulty.\nLearn more about the most important changes here:\nhttps://github.com/openai/gym and here: https://github.com/Farama-Foundation/Gymnasium\n\nIn order to fix this problem, do the following:\n\n1) Run `pip install gymnasium` on your command line.\n2) Change all your import statements in your code from\n   `import gym` -> `import gymnasium as gym` OR\n   `from gym.space import Discrete` -> `from gymnasium.spaces import Discrete`\n\nFor your custom (single agent) gym.Env classes:\n3.1) Either wrap your old Env class via the provided `from gymnasium.wrappers import\n     EnvCompatibility` wrapper class.\n3.2) Alternatively to 3.1:\n - Change your `reset()` method to have the call signature 'def reset(self, *,\n   seed=None, options=None)'\n - Return an additional info dict (empty dict should be fine) from your `reset()`\n   method.\n - Return an additional `truncated` flag from your `step()` method (between `done` and\n   `info`). This flag should indicate, whether the episode was terminated prematurely\n   due to some time constraint or other kind of horizon setting.\n\nFor your custom RLlib `MultiAgentEnv` classes:\n4.1) Either wrap your old MultiAgentEnv via the provided\n     `from ray.rllib.env.wrappers.multi_agent_env_compatibility import\n     MultiAgentEnvCompatibility` wrapper class.\n4.2) Alternatively to 4.1:\n - Change your `reset()` method to have the call signature\n   'def reset(self, *, seed=None, options=None)'\n - Return an additional per-agent info dict (empty dict should be fine) from your\n   `reset()` method.\n - Rename `dones` into `terminateds` and only set this to True, if the episode is really\n   done (as opposed to has been terminated prematurely due to some horizon/time-limit\n   setting).\n - Return an additional `truncateds` per-agent dictionary flag from your `step()`\n   method, including the `__all__` key (100% analogous to your `dones/terminateds`\n   per-agent dict).\n   Return this new `truncateds` dict between `dones/terminateds` and `infos`. This\n   flag should indicate, whether the episode (for some agent or all agents) was\n   terminated prematurely due to some time constraint or other kind of horizon setting.\n\n\nThe above error has been found in your environment! We've added a module for checking your custom environments. It may cause your experiment to fail if your environment is not set up correctly. You can disable this behavior via calling `config.environment(disable_env_checking=True)`. You can run the environment checking module standalone by calling ray.rllib.utils.check_env([your env])."
     ]
    }
   ],
   "source": [
    "def evaluate(agent, env):\n",
    "    obs, info = env.reset(seed=42)\n",
    "    terminated = False\n",
    "\n",
    "    # Initialize data tracking\n",
    "    profit_loss = defaultdict(float)\n",
    "    total_profit_loss = defaultdict(float)\n",
    "    stock_prices = defaultdict(list)\n",
    "    transaction_volumes = defaultdict(list)\n",
    "    portfolio_values = defaultdict(list)\n",
    "    portfolio_stock_number = defaultdict(list)\n",
    "    profit_loss_values = defaultdict(list)\n",
    "    total_values = []\n",
    "\n",
    "    # Calculate the starting portfolio value\n",
    "    stocks = env.stock_data.columns\n",
    "    total_value_start = env.cash + sum(env.positions[stock] * env.stock_data.loc[env.current_step, stock] for stock in stocks)\n",
    "    total_values.append(total_value_start)\n",
    "    portfolio_ratios = {stock: [] for stock in stocks}\n",
    "    portfolio_ratios[\"cash\"] = []\n",
    "\n",
    "    while not terminated:\n",
    "        # action = agent.compute_action(obs)\n",
    "        action = agent.compute_single_action(obs, exploit=True)  # force deterministic actions\n",
    "        obs, reward, terminated, _, info = env.step(action)\n",
    "\n",
    "        # Update the profit/loss for each stock\n",
    "        for i, stock in enumerate(stocks):\n",
    "            stock_prices[stock].append(env.stock_data.loc[env.current_step, stock])\n",
    "            transaction_volumes[stock].append(env.shares_amounts[action[i]])\n",
    "            portfolio_stock_number[stock].append(env.positions[stock])\n",
    "            current_value = env.positions[stock] * env.stock_data.loc[env.current_step, stock]\n",
    "            portfolio_values[stock].append(current_value)\n",
    "            cost = info.get(stock, {}).get('cost', 0)\n",
    "            sold_value = info.get(stock, {}).get('sell_value', 0)\n",
    "            profit_loss[stock] += sold_value - cost\n",
    "            total_profit_loss[stock] = profit_loss[stock] + current_value\n",
    "            profit_loss_values[stock].append(total_profit_loss[stock])\n",
    "            \n",
    "        # Update portfolio ratios\n",
    "        total_value = env.cash + sum(env.positions[stock] * env.stock_data.loc[env.current_step, stock] for stock in stocks)\n",
    "        for stock in stocks:\n",
    "            portfolio_ratios[stock].append(portfolio_values[stock][-1] / total_value)\n",
    "        portfolio_ratios[\"cash\"].append(env.cash / total_value)\n",
    "\n",
    "\n",
    "        total_value_end = env.cash + sum(env.positions[stock] * env.stock_data.loc[env.current_step, stock] for stock in stocks)\n",
    "        total_values.append(total_value_end)\n",
    "\n",
    "    # Calculate total value percentage change\n",
    "    total_value_percentage_change = ((total_value_end - total_value_start) / total_value_start) * 100\n",
    "    total_values_percentage = [(value / total_value_start - 1) * 100 for value in total_values]\n",
    "    \n",
    "    # Plot data\n",
    "    for stock in stocks:\n",
    "        plt.figure(figsize=(12, 9))\n",
    "\n",
    "        plt.subplot(511)\n",
    "        plt.plot(stock_prices[stock])\n",
    "        plt.title(f\"Price of {stock}\")\n",
    "\n",
    "        plt.subplot(512)\n",
    "        plt.plot(transaction_volumes[stock])\n",
    "        plt.title(f\"Transaction volume of {stock}\")\n",
    "        \n",
    "        plt.subplot(513)\n",
    "        plt.plot(portfolio_stock_number[stock])\n",
    "        plt.title(f\"Portfolio stock number of {stock}\")\n",
    "\n",
    "        plt.subplot(514)\n",
    "        plt.plot(portfolio_values[stock])\n",
    "        plt.title(f\"Portfolio value of {stock}\")\n",
    "\n",
    "        plt.subplot(515)\n",
    "        plt.plot(profit_loss_values[stock])\n",
    "        plt.title(f\"Profit/Loss of {stock}\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(modelpath+f'/fig/{stock}_result.png')\n",
    "        plt.show()\n",
    "\n",
    "    plt.figure(figsize=(12, 3))\n",
    "    plt.plot(total_values_percentage)\n",
    "    plt.title(\"Total asset value (% change from start)\")\n",
    "    plt.savefig(modelpath+'/fig/asset_transition.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot portfolio ratios\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    bar_width = 0.85\n",
    "    bar_l = [i+1 for i in range(len(portfolio_ratios['cash']))] \n",
    "    tick_pos = [i+(bar_width/2) for i in bar_l]\n",
    "\n",
    "    # Create a total ratio for stacking\n",
    "    total_ratio = [0] * len(portfolio_ratios['cash'])\n",
    "\n",
    "    for stock, ratios in portfolio_ratios.items():\n",
    "        plt.bar(bar_l, ratios, bottom=total_ratio, label=stock, width=bar_width)\n",
    "        total_ratio = [i+j for i,j in zip(total_ratio, ratios)]\n",
    "\n",
    "    # Here we set the x-ticks to only include every nth tick to prevent crowding.\n",
    "    n = 200  # the number of steps between each tick\n",
    "    plt.xticks(tick_pos[::n], range(len(portfolio_ratios['cash']))[::n])\n",
    "\n",
    "    plt.legend()\n",
    "    plt.title(\"Portfolio asset ratios over time\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Asset ratio\")\n",
    "    plt.savefig(modelpath+'/fig/ratio_transition.png')\n",
    "    plt.show()\n",
    "\n",
    "    return total_value_percentage_change, dict(total_profit_loss)\n",
    "\n",
    "\n",
    "\n",
    "register_env(env_name, create_stock_trading_env)\n",
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "# Create the environment\n",
    "env = StockTradingEnv(stock_data, features)\n",
    "\n",
    "# Create PPO agent\n",
    "ppo_config = PPOConfig()\n",
    "ppo_config[\"seed\"] = 42\n",
    "ppo_agent = ppo_config.environment(env=env_name)\n",
    "ppo_agent = ppo_config.build()\n",
    "ppo_agent.restore(modelpath)\n",
    "\n",
    "print(\"\\nEvaluating trained agents:\")\n",
    "rewards = evaluate(ppo_agent, env)\n",
    "print(f\"Total value percentage change: {rewards[0]}\")\n",
    "for stock, reward in rewards[1].items():\n",
    "    print(f\"PPO: {stock} profit/loss={reward}\")\n",
    "\n",
    "\n",
    "# Shutdown Ray\n",
    "ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
